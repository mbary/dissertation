{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'database_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-361ca5cceb2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# interactive mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatabase_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabasereader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatabaseReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Custom Functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'database_functions'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import collections\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import time\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from database_functions.databasereader import DatabaseReader\n",
    "\n",
    "# Custom Functions\n",
    "from shoe_dataset import ShoeDataSet\n",
    "from shoe_dataset import train_transformations, valid_transformations\n",
    "from feature_maps_extractor import ExtractFeatureMaps\n",
    "from class_activation_maps import ScoreCam\n",
    "\n",
    "torch.manual_seed(1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_model:\n",
    "    \n",
    "    def __init__(self, model_type,unfreeze_layers, output_dir,freeze_factor = 1 , pretrained = True):\n",
    "        \n",
    "        if torch.cuda.is_available():      \n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "            print(\"Model set up on the GPU\")\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            print(\"Running on the CPU\")\n",
    "        \n",
    "        \n",
    "        # Define the backbone model\n",
    "        if model_type == \"resnet101\":\n",
    "            self.model = models.resnet101(pretrained=pretrained)\n",
    "        \n",
    "        elif model_type == \"resnet50\":\n",
    "            self.model = models.resnet50(pretrained=pretrained)        \n",
    "        \n",
    "        else: \n",
    "            print(\"model type unrecognised\")\n",
    "            \n",
    "        print(f\"Model backbone set to: {model_type}\")\n",
    "        \n",
    "        # Push the model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.unfreeze_layers = unfreeze_layers\n",
    "        self.freeze_factor = freeze_factor\n",
    "        \n",
    "        if self.freeze_factor == 1:\n",
    "            for name,child in self.model.named_children():\n",
    "                if name not in self.unfreeze_layers:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = False\n",
    "        \n",
    "        else:\n",
    "            for name,child in self.model.named_children():\n",
    "                if name not in self.unfreeze_layers:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_gra = False\n",
    "                    \n",
    "                elif name in self.unfreeze_layers:    \n",
    "                    for param in list(child.parameters())[int(self.freeze_factor*len(list(child.parameters()))):]:\n",
    "                        param.requires_grad = False\n",
    "\n",
    "                        \n",
    "        # Defines the output dir for the model to be saved\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        self.writer = SummaryWriter(self.output_dir,)\n",
    "            \n",
    "    \n",
    "    def train(self, loss_func, optimizer, lr_scheduler,learning_rate, epochs, trainloader, valloader, eval_period = 100 ):\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eval_period = eval_period\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        \n",
    "        # Params of the 4th layer to confirm later \n",
    "        # that layers are correctly frozen\n",
    "        self._original_weight = list(self.model.layer4.parameters())[0]\n",
    "        \n",
    "        \n",
    "        print(f\"Eval Period set to: {self.eval_period}\")\n",
    "\n",
    "        self.criterion = loss_func()\n",
    "        self.optimizer = optimizer(filter(lambda p: p.requires_grad, self.model.parameters()),lr = self.learning_rate)\n",
    "        self.lr_scheduler = lr_scheduler(self.optimizer)\n",
    "\n",
    "        train_epoch_loss = []\n",
    "        train_epoch_acc  = []\n",
    "\n",
    "        val_epoch_loss = []\n",
    "        val_epoch_acc = []\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        running_training_loss = 0.0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # check for first 3 epochs whether the layers\n",
    "            # are actually frozen and the weights do not change\n",
    "            \n",
    "            if 0 < epoch < 3:\n",
    "                \n",
    "                print(f\"Did the Weights change in epoch: {epoch}?\")\n",
    "                if list(self.model.layer4.parameters())[0].cpu().numpy().all() == self._original_weight[0].cpu().numpy().all():\n",
    "                    print(\"No\")\n",
    "                else:\n",
    "                    print(\"Yes, abort!!!\")\n",
    "            \n",
    "            \n",
    "\n",
    "            train_batch_loss = []\n",
    "            train_batch_acc = []\n",
    "\n",
    "            val_batch_loss = []\n",
    "            val_batch_acc = []\n",
    "\n",
    "            # train loop\n",
    "            for idx, data in enumerate(self.trainloader):\n",
    "\n",
    "                self.model.train()\n",
    "                \n",
    "                inputs,labels = data[\"image\"],data[\"label\"]\n",
    "                labels = np.asarray(labels)\n",
    "                labels = torch.from_numpy(labels.astype(\"long\"))\n",
    "#                 print(type(inputs))\n",
    "#                 print(labels)\n",
    "                # Place tensors on GPU\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Zero out the accumulated gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                outputs = outputs#.permute(1,0) \n",
    "#                 print(\"outputs shape\",outputs.shape)\n",
    "#                 print(\"lables shape\", labels.shape)\n",
    "                loss = self.criterion(outputs,labels)#.float())\n",
    "                \n",
    "                running_training_loss+=loss.item()\n",
    "\n",
    "                #append the mean train loss (woking on a batch)\n",
    "                #use item() to detach from GPU\n",
    "                train_batch_loss.append(loss.item())\n",
    "\n",
    "                # Identify Correct predictions\n",
    "                correct_preds = [torch.argmax(i)==torch.argmax(j) for i,j in zip(outputs,labels)]\n",
    "                train_acc = correct_preds.count(True)/len(correct_preds)\n",
    "                train_batch_acc.append(train_acc)\n",
    "\n",
    "\n",
    "                # Backwards pass\n",
    "                loss.backward()\n",
    "\n",
    "\n",
    "                self.optimizer.step()                                                                \n",
    "                \n",
    "                # print out stats every N iterations\n",
    "                if idx%200 == 0:\n",
    "                    \n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Current lr: {current_lr}\")\n",
    "                    print(f\"Step: {idx}/{len(trainloader)}; Epoch: {epoch+1}/{self.epochs}; Train Batch Loss: {loss.item()}\")\n",
    "            \n",
    "            \n",
    "            self.writer.add_scalar(\"Training Loss\", running_training_loss/len(self.trainloader), epoch)\n",
    "            running_training_loss = 0.0\n",
    "            \n",
    "            train_epoch_loss.append(torch.tensor(train_batch_loss).mean())\n",
    "            train_epoch_acc.append(torch.tensor(train_batch_acc).mean())\n",
    "\n",
    "            running_val_loss = 0\n",
    "\n",
    "            # Validation loop every self.eval_period epochs\n",
    "            eval_count = 0\n",
    "            if epoch%self.eval_period == 0:\n",
    "                print(f\"Starting evaluating at epoch: {epoch}\")\n",
    "                with torch.no_grad():\n",
    "                    for idx,data in enumerate(self.valloader):\n",
    "\n",
    "                        # set model in eval() mode\n",
    "                        self.model.eval()\n",
    "\n",
    "                        inputs,labels, self.val_file_name = data[\"image\"], data[\"label\"], data[\"file_name\"]\n",
    "                        labels = np.asarray(labels)\n",
    "                        labels = torch.from_numpy(labels.astype(\"long\"))\n",
    "                        # Remove inputs/lables from the GPU\n",
    "                        inputs = inputs.detach().cpu()\n",
    "                        labels = labels.detach().cpu()\n",
    "\n",
    "                        # Predict outputs\n",
    "                        outputs = self.model(inputs)\n",
    "                        outputs = outputs  \n",
    "#                         print(\"outputs shape\", outputs.shape)\n",
    "#                         print(\"labels shape\", labels.shape)\n",
    "                        # Obtain and append val_batch_loss\n",
    "                        val_loss = self.criterion(outputs,labels)\n",
    "                        val_batch_loss.append(loss.item())\n",
    "                        running_val_loss += val_loss\n",
    "\n",
    "                        # Obtain and append val_batch_acc\n",
    "                        correct_preds = [torch.argmax(i) == torch.argmax(j) for i,j in zip(outputs,labels)]\n",
    "\n",
    "                        val_acc = correct_preds.count(True)/len(correct_preds)\n",
    "                        val_batch_acc.append(val_acc)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        if eval_count%3 == 0:\n",
    "                            \n",
    "#                             all_feature_maps = []\n",
    "#                             extractor = ExtractFeatureMaps(self.model)\n",
    "#                             print(\"collecting feature maps\")\n",
    "# #                             for name in data[\"file_name\"][:1]:\n",
    "# #                                 print(\"name\", name)\n",
    "# #                                 extracteds_map = extractor.extraction(name)\n",
    "# #                                 all_feature_maps.append(torch.from_numpy(extracteds_map))\n",
    "                            \n",
    "#                             maps = extractor.extraction(data[\"file_name\"][0])\n",
    "#                             print(\"len  maps\",len(maps))\n",
    "#                             print(\"type map\", type(maps))\n",
    "#                             print(\"dsshape maps\", maps.shape)\n",
    "#                             maps_tp = np.moveaxis(maps, 3,1)\n",
    "# #                             maps_tens = torch.from_numpy(maps)\n",
    "# #                             maps_tens = maps_tens.permute(0,3,2,1)\n",
    "# #                             print(\"maps_tens shape\", maps_tens.shape)\n",
    "                            \n",
    "# #                             maps = maps.permute(2,1,0)\n",
    "# #                             maps_tensor = torch.from_numpy(maps)\n",
    "# #                             all_feature_maps_stacked = np.vstack(all_feature_maps)\n",
    "# #                             grid = torchvision.utils.make_grid(all_feature_maps)\n",
    "# #                             all_feature_maps_stacked = torch.from_numpy(all_feature_maps_stacked)\n",
    "#                             self.writer.add_images(\"Feature Maps of every 10th conv layer\", maps_tp,global_step=epoch)\n",
    "                                \n",
    "                            \n",
    "                            all_layers_maps = []\n",
    "                            print(\"Preparing CAM\")\n",
    "                            \n",
    "                            for i in range(1,5):\n",
    "                                print(f\"Layer{i}\")\n",
    "                                score_cam = ScoreCam(self.model, f\"layer{i}\")\n",
    "\n",
    "                                eval_count+=1\n",
    "\n",
    "                                top_images = []\n",
    "                                bottom_images = []\n",
    "                                images_list = []\n",
    "\n",
    "                                images = data[\"image\"]\n",
    "                                names = data[\"file_name\"]\n",
    "\n",
    "                                for idx, (image,name) in enumerate(zip(images,names),1):\n",
    "#                                     image = torch.unsqueeze(image,0)\n",
    "                                    image = torch.unsqueeze(image,0)\n",
    "                                    no_trans, heatmap_image = score_cam.generate_cam(input_image=image, filename=name)\n",
    "                                    images_list.append(np.array(heatmap_image))\n",
    "#                                     if idx <= int(len(data)/2):\n",
    "#                                         top_images.append(np.array(heatmap_image))\n",
    "                                        \n",
    "#                                     else:\n",
    "#                                         bottom_images.append(np.array(heatmap_image))\n",
    "                                        \n",
    "#                                 top_images = np.hstack(top_images)\n",
    "#                                 print(\"shape of top im\", top_images.shape)\n",
    "#                                 bottom_images = np.hstack(bottom_images)\n",
    "#                                 print(\"shape of bottom\", bottom_images.shape)\n",
    "#                                 all_images = np.vstack((top_images,bottom_images))\n",
    "                                \n",
    "                                all_layers_maps.append(images_list)\n",
    "                            \n",
    "                            all_layers_maps = np.hstack(all_layers_maps)\n",
    "#                             print(\"all layers maps shaspe\", all_layers_maps.shape)\n",
    "#                             print(\"all layers maps type\", type(all_layers_maps))\n",
    "                            all_layers_maps = np.moveaxis(all_layers_maps, -1,1)\n",
    "                            all_layers_maps = np.moveaxis(all_layers_maps, -1,-2)\n",
    "#                             print(\"\\n all_layers_maps[:,:,0,0]\", all_layers_maps[:,:,0,0],\"\\n\")\n",
    "#                             print(\"\\n all_layers_maps[:,:,1,1]\",all_layers_maps[:,:,1,1],\"\\n\")\n",
    "#                             print(\"\\n all_layers_maps[:,:,2,2]\",all_layers_maps[:,:,2,2],\"\\n\")\n",
    "#                             print(\"\\n all_layers_maps[:,0,:,:]\",all_layers_maps[:,0,:,:],\"\\n\")\n",
    "#                             print(\"\\n all_layers_maps[:,1,:,:]\",all_layers_maps[:,1,:,:],\"\\n\")\n",
    "#                             print(\"\\n all_layers_maps[:,2,:,:]\",all_layers_maps[:,2,:,:],\"\\n\")\n",
    "#                             print(\"\\n all_layers_maps[:,3,:,:]\",all_layers_maps[:,3,:,:].shape,\"\\n\")\n",
    "#                             print(\"any != 255\", all_layers_maps[:,3,:,:].any() != 255)\n",
    "#                             print(\"all == 255\", all_layers_maps[:,3,:,:].all()==255)\n",
    "                            all_layers_maps = all_layers_maps[:,:3,:,:]\n",
    "#                             print(\"shape after slicing\", all_layers_maps.shape)\n",
    "                            \n",
    "#                             print(\"\\n all layers maps shaspe\", all_layers_maps.shape)\n",
    "\n",
    "                            self.writer.add_images(\"Class Activation Maps, Layers 1-4\", torch.from_numpy(all_layers_maps), global_step = epoch)\n",
    "                                \n",
    "#                             print(\"******* \\n\\n\\n COMPLETED CAM KURWA\\n\\n\\n **********\")\n",
    "                                \n",
    "                                \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "\n",
    "                    val_epoch_loss.append(torch.tensor(val_batch_loss).mean())\n",
    "                    val_epoch_acc.append(torch.tensor(val_batch_acc).mean())\n",
    "                    \n",
    "                    \n",
    "                    # Visualise the predictions on the last validation batch\n",
    "                    \n",
    "#                     print(f\"len inputs of last batch: {len(inputs)}\")\n",
    "                    \n",
    "                    self.writer.add_scalar(\"Validation Loss\", \n",
    "                                           running_val_loss/len(self.valloader),\n",
    "                                           global_step = epoch\n",
    "                                          )\n",
    "                    \n",
    "                    self.writer.add_figure(\"Predictions vs. GT\",\n",
    "                                           self.plot_classes_preds(inputs, labels),\n",
    "                                           global_step = epoch\n",
    "                                          )\n",
    "                    \n",
    "                    running_val_loss = 0.0\n",
    "                    \n",
    "                    \n",
    "            # Save the model which yielding best acc\n",
    "            if val_acc > best_val_acc:\n",
    "                print(f\"Saving model at epoch: {epoch}\")\n",
    "                best_val_acc = val_acc\n",
    "                self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "            # Print out \n",
    "\n",
    "            self.lr_scheduler.step(val_acc)\n",
    "\n",
    "\n",
    "        self.model = self.model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def matplotlib_imshow(self, img, one_channel=False):\n",
    "        if one_channel:\n",
    "            img = img.mean(dim=0)\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.cpu().numpy()\n",
    "        if one_channel:\n",
    "            plt.imshow(npimg, cmap=\"Greys\")\n",
    "        else:\n",
    "            plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "            \n",
    "    def images_to_probs(self, images):\n",
    "        '''\n",
    "        Generates predictions and corresponding probabilities from a trained\n",
    "        network and a list of images\n",
    "        '''\n",
    "        output = self.model(images)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, preds_tensor = torch.max(output, 1)\n",
    "        preds = np.squeeze(preds_tensor.cpu().numpy())\n",
    "        return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "    def plot_classes_preds(self, images, labels):\n",
    "        '''\n",
    "        Generates matplotlib Figure using a trained network, along with images\n",
    "        and labels from a batch, that shows the network's top prediction along\n",
    "        with its probability, alongside the actual label, coloring this\n",
    "        information based on whether the prediction was correct or not.\n",
    "        Uses the \"images_to_probs\" function.\n",
    "        '''\n",
    "        labels = labels.detach().cpu()\n",
    "        with torch.no_grad():\n",
    "            preds, probs = self.images_to_probs(images)\n",
    "            # plot the images in the batch, along with predicted and true labels\n",
    "            fig = plt.figure(figsize=(48, 15))\n",
    "            for idx in np.arange(4):\n",
    "                print(f\"pred idx: {preds[idx]}\")\n",
    "                ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "                self.matplotlib_imshow(images[idx], one_channel=False)\n",
    "                ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "                    classes[preds[idx]],\n",
    "                    probs[idx] * 100.0,\n",
    "                    classes[labels[idx].item()]),\n",
    "                            color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"), fontsize=40)\n",
    "            return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Import all data from the database\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "db_reader = DatabaseReader(\"crepcheque\")\n",
    "\n",
    "get_images_query =\"\"\"\n",
    "\n",
    "                    SELECT\n",
    "                        r.crep_id,\n",
    "                        r.raw_brand_text,\n",
    "                        i.image_id,\n",
    "                        i.image_type,\n",
    "                        i.image_file_path\n",
    "                    FROM raw_creps r\n",
    "                    LEFT JOIN\n",
    "                        images i\n",
    "                        ON r.crep_id = i.crep_id\n",
    "                    WHERE \n",
    "                        r.images IS NOT NULL\n",
    "                        AND r.images_processed = true\n",
    "                        AND i.image_downloaded = true\n",
    "                    \"\"\"\n",
    "\n",
    "database_df = db_reader.send_query(query=get_images_query, return_as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328852, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_counts = pd.DataFrame(database_df.raw_brand_text.value_counts())\n",
    "brand_counts.reset_index(inplace=True)\n",
    "brand_counts.columns = ['brand', 'count']\n",
    "brands_to_keep = brand_counts[brand_counts['count'] >= 10].brand.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_db_df = database_df[database_df.raw_brand_text.isin(brands_to_keep)]\n",
    "filtered_db_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_db_data(brand, db_df, n=3000):\n",
    "    sampled_df = db_df[db_df.raw_brand_text == brand]\n",
    "    #print(sampled_df.shape)\n",
    "    shuffled_sampled_df = sklearn.utils.shuffle(sampled_df)\n",
    "    shuffled_sampled_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    if n > shuffled_sampled_df.shape[0]:\n",
    "        final_sampled = shuffled_sampled_df\n",
    "    else:\n",
    "        final_sampled = shuffled_sampled_df.loc[0:n, :]\n",
    "    \n",
    "    return final_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = 'Gucci'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_df_list = []\n",
    "for brand in brands_to_keep:\n",
    "    sampled_df_list.append(sample_db_data(brand=brand, db_df=filtered_db_df, n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_filtered_df = pd.concat(sampled_df_list,axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(downsampled_filtered_df.raw_brand_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_filtered_df[\"encoded_label\"] = label_encoder.transform(downsampled_filtered_df.raw_brand_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crep_id</th>\n",
       "      <th>raw_brand_text</th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_type</th>\n",
       "      <th>image_file_path</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17233</td>\n",
       "      <td>Nike</td>\n",
       "      <td>175179</td>\n",
       "      <td>Additional</td>\n",
       "      <td>/home/max/hdd-data1/images/original/17233-1751...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14465</td>\n",
       "      <td>Nike</td>\n",
       "      <td>147997</td>\n",
       "      <td>Additional</td>\n",
       "      <td>/home/max/hdd-data1/images/original/14465-1479...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5555</td>\n",
       "      <td>Nike</td>\n",
       "      <td>56005</td>\n",
       "      <td>Additional</td>\n",
       "      <td>/home/max/hdd-data1/images/original/5555-56005...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9046</td>\n",
       "      <td>Nike</td>\n",
       "      <td>92204</td>\n",
       "      <td>Additional</td>\n",
       "      <td>/home/max/hdd-data1/images/original/9046-92204...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9087</td>\n",
       "      <td>Nike</td>\n",
       "      <td>92659</td>\n",
       "      <td>Additional</td>\n",
       "      <td>/home/max/hdd-data1/images/original/9087-92659...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   crep_id raw_brand_text  image_id  image_type  \\\n",
       "0    17233           Nike    175179  Additional   \n",
       "1    14465           Nike    147997  Additional   \n",
       "2     5555           Nike     56005  Additional   \n",
       "3     9046           Nike     92204  Additional   \n",
       "4     9087           Nike     92659  Additional   \n",
       "\n",
       "                                     image_file_path  encoded_label  \n",
       "0  /home/max/hdd-data1/images/original/17233-1751...             38  \n",
       "1  /home/max/hdd-data1/images/original/14465-1479...             38  \n",
       "2  /home/max/hdd-data1/images/original/5555-56005...             38  \n",
       "3  /home/max/hdd-data1/images/original/9046-92204...             38  \n",
       "4  /home/max/hdd-data1/images/original/9087-92659...             38  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{38: 'Nike',\n",
       " 26: 'Jordan',\n",
       " 63: 'adidas',\n",
       " 60: 'Vans',\n",
       " 37: 'New Balance',\n",
       " 14: 'Converse',\n",
       " 0: 'ASICS',\n",
       " 48: 'Reebok',\n",
       " 47: 'Puma',\n",
       " 5: 'Balenciaga',\n",
       " 59: 'Under Armour',\n",
       " 53: 'Saucony',\n",
       " 57: 'Timberland',\n",
       " 17: 'Dior',\n",
       " 4: 'BAPE',\n",
       " 16: 'Diadora',\n",
       " 22: 'Gucci',\n",
       " 39: 'OFF-WHITE',\n",
       " 12: 'Clarks',\n",
       " 33: 'Louis Vuitton',\n",
       " 62: 'Yeezy',\n",
       " 32: 'Li-Ning',\n",
       " 27: 'K-Swiss',\n",
       " 18: 'Dr. Martens',\n",
       " 19: 'Ewing Athletics',\n",
       " 20: 'FEAR OF GOD',\n",
       " 11: 'Chanel',\n",
       " 15: 'DC Shoes',\n",
       " 61: 'Versace',\n",
       " 21: 'Fila',\n",
       " 23: 'Hoka One One',\n",
       " 54: 'Sonra',\n",
       " 28: 'KangaROOS',\n",
       " 7: 'Birkenstock',\n",
       " 55: 'Suicoke',\n",
       " 58: 'Tommy Hilfiger',\n",
       " 3: 'Asics',\n",
       " 40: 'Off-White',\n",
       " 13: 'Common Projects',\n",
       " 29: 'Karhu',\n",
       " 45: 'Prada',\n",
       " 41: 'Onitsuka Tiger',\n",
       " 1: 'Alexander McQueen',\n",
       " 24: 'Hummel',\n",
       " 56: 'Supra',\n",
       " 6: 'Big Baller Brand',\n",
       " 34: 'Mephisto',\n",
       " 44: 'Pizza Hut',\n",
       " 25: 'Ice Cream',\n",
       " 2: 'Anta',\n",
       " 30: 'Lakai',\n",
       " 43: 'Palace',\n",
       " 42: 'Osiris',\n",
       " 49: 'Revenge X Storm',\n",
       " 50: 'Rhude',\n",
       " 8: 'Brandblack',\n",
       " 10: 'Burberry',\n",
       " 52: 'Salomon',\n",
       " 51: 'Saint Laurent',\n",
       " 36: 'Mizuno',\n",
       " 31: 'Le Coq Sportif',\n",
       " 46: 'Pro Keds',\n",
       " 35: 'Mercer',\n",
       " 64: 'es',\n",
       " 9: 'Brooks'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoding_dict = dict(list(zip(downsampled_filtered_df[\"encoded_label\"].tolist(),downsampled_filtered_df[\"raw_brand_text\"].tolist())))\n",
    "labels_encoding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, 'ASICS'),\n",
       "             (1, 'Alexander McQueen'),\n",
       "             (2, 'Anta'),\n",
       "             (3, 'Asics'),\n",
       "             (4, 'BAPE'),\n",
       "             (5, 'Balenciaga'),\n",
       "             (6, 'Big Baller Brand'),\n",
       "             (7, 'Birkenstock'),\n",
       "             (8, 'Brandblack'),\n",
       "             (9, 'Brooks'),\n",
       "             (10, 'Burberry'),\n",
       "             (11, 'Chanel'),\n",
       "             (12, 'Clarks'),\n",
       "             (13, 'Common Projects'),\n",
       "             (14, 'Converse'),\n",
       "             (15, 'DC Shoes'),\n",
       "             (16, 'Diadora'),\n",
       "             (17, 'Dior'),\n",
       "             (18, 'Dr. Martens'),\n",
       "             (19, 'Ewing Athletics'),\n",
       "             (20, 'FEAR OF GOD'),\n",
       "             (21, 'Fila'),\n",
       "             (22, 'Gucci'),\n",
       "             (23, 'Hoka One One'),\n",
       "             (24, 'Hummel'),\n",
       "             (25, 'Ice Cream'),\n",
       "             (26, 'Jordan'),\n",
       "             (27, 'K-Swiss'),\n",
       "             (28, 'KangaROOS'),\n",
       "             (29, 'Karhu'),\n",
       "             (30, 'Lakai'),\n",
       "             (31, 'Le Coq Sportif'),\n",
       "             (32, 'Li-Ning'),\n",
       "             (33, 'Louis Vuitton'),\n",
       "             (34, 'Mephisto'),\n",
       "             (35, 'Mercer'),\n",
       "             (36, 'Mizuno'),\n",
       "             (37, 'New Balance'),\n",
       "             (38, 'Nike'),\n",
       "             (39, 'OFF-WHITE'),\n",
       "             (40, 'Off-White'),\n",
       "             (41, 'Onitsuka Tiger'),\n",
       "             (42, 'Osiris'),\n",
       "             (43, 'Palace'),\n",
       "             (44, 'Pizza Hut'),\n",
       "             (45, 'Prada'),\n",
       "             (46, 'Pro Keds'),\n",
       "             (47, 'Puma'),\n",
       "             (48, 'Reebok'),\n",
       "             (49, 'Revenge X Storm'),\n",
       "             (50, 'Rhude'),\n",
       "             (51, 'Saint Laurent'),\n",
       "             (52, 'Salomon'),\n",
       "             (53, 'Saucony'),\n",
       "             (54, 'Sonra'),\n",
       "             (55, 'Suicoke'),\n",
       "             (56, 'Supra'),\n",
       "             (57, 'Timberland'),\n",
       "             (58, 'Tommy Hilfiger'),\n",
       "             (59, 'Under Armour'),\n",
       "             (60, 'Vans'),\n",
       "             (61, 'Versace'),\n",
       "             (62, 'Yeezy'),\n",
       "             (63, 'adidas'),\n",
       "             (64, 'es')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoding_dict_sorted = collections.OrderedDict(sorted(labels_encoding_dict.items()))\n",
    "labels_encoding_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = labels_encoding_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Shuffle the dataframe and split into train and validation\n",
    "\n",
    "\"\"\";\n",
    "\n",
    "# shuffled_df = sklearn.utils.shuffle(database_df)\n",
    "\n",
    "# train_df = shuffled_df.iloc[:int(database_df.shape[0]*0.8), :]\n",
    "# val_df = shuffled_df.iloc[int(database_df.shape[0]*0.8):, :]\n",
    "\n",
    "X = downsampled_filtered_df.image_file_path#.tolist()\n",
    "y = downsampled_filtered_df.encoded_label#.tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2 ,stratify = y, \n",
    "                                                    random_state = 1994)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train],axis=1)\n",
    "val_df = pd.concat([X_val,y_val], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Define the image transformatoins for the train and validation sets\n",
    "Define \n",
    "\n",
    "\"\"\"\n",
    "transforms_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomPerspective(p = 0.4), # randomly change img perspective\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = 0.2),\n",
    "    torchvision.transforms.Resize((300,300)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,0.5,0.5,),(0.5,0.5,0.5)),\n",
    "])\n",
    "transforms_valid = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((300,300)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,0.5,0.5,),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "train_set = ShoeDataSet(train_df, transform=transforms_train)\n",
    "val_set = ShoeDataSet(val_df, transform=transforms_valid)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=4,drop_last=True)\n",
    "val_loader = DataLoader(val_set, shuffle=True, batch_size=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set up on the GPU\n",
      "Model backbone set to: resnet101\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Add layers which are to be unfrozen for finetuning purposes\n",
    "\n",
    "Options: fc (bare minimum), layer1-4 (conv2d layer bottlenecks)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_model = custom_model(\"resnet101\",[\"fc\"], output_dir=\"../runs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Redefine the FC layer of your model so that it matches \n",
    "the number of classes present in the dataset\n",
    "\n",
    "Ensure that it is placed on the GPU - .cuda()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_model.model.fc = torch.nn.Linear(2048, len(set(y))).cuda()\n",
    "\n",
    "# Define the output directory for the logs\n",
    "custom_model.output_dir = \"../runs/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../runs/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Define the loss functoin (criterion)\n",
    "OPtimizer - Adam\n",
    "LRScheduler\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam\n",
    "learning_rate = 0.001\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Period set to: 10\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 1/100; Train Batch Loss: 4.212957859039307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/lib/python3.6/site-packages/PIL/Image.py:961: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluating at epoch: 0\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 63\n",
      "pred idx: 46\n",
      "pred idx: 46\n",
      "pred idx: 46\n",
      "Did the Weights change in epoch: 1?\n",
      "No\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 2/100; Train Batch Loss: 3.9643640518188477\n",
      "Did the Weights change in epoch: 2?\n",
      "No\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 3/100; Train Batch Loss: 4.322152137756348\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 4/100; Train Batch Loss: 4.403522491455078\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 5/100; Train Batch Loss: 3.9957828521728516\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 6/100; Train Batch Loss: 4.210690498352051\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 7/100; Train Batch Loss: 4.2820515632629395\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 8/100; Train Batch Loss: 4.463751792907715\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 9/100; Train Batch Loss: 4.358029365539551\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 10/100; Train Batch Loss: 4.182094573974609\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 11/100; Train Batch Loss: 4.084150791168213\n",
      "Starting evaluating at epoch: 10\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 64\n",
      "pred idx: 57\n",
      "pred idx: 57\n",
      "pred idx: 28\n",
      "Current lr: 0.001\n",
      "Step: 0/142; Epoch: 12/100; Train Batch Loss: 4.147579193115234\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 13/100; Train Batch Loss: 4.054973602294922\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 14/100; Train Batch Loss: 4.4361186027526855\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 15/100; Train Batch Loss: 4.293096542358398\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 16/100; Train Batch Loss: 4.140508651733398\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 17/100; Train Batch Loss: 4.352625370025635\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 18/100; Train Batch Loss: 4.407443523406982\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 19/100; Train Batch Loss: 4.245572090148926\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 20/100; Train Batch Loss: 4.259666442871094\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 21/100; Train Batch Loss: 4.37302827835083\n",
      "Starting evaluating at epoch: 20\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 34\n",
      "pred idx: 2\n",
      "pred idx: 2\n",
      "pred idx: 31\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 22/100; Train Batch Loss: 4.498281002044678\n",
      "Current lr: 0.0001\n",
      "Step: 0/142; Epoch: 23/100; Train Batch Loss: 4.218445777893066\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 24/100; Train Batch Loss: 4.090913772583008\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 25/100; Train Batch Loss: 3.9874868392944336\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 26/100; Train Batch Loss: 4.29325008392334\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 27/100; Train Batch Loss: 4.22089958190918\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 28/100; Train Batch Loss: 4.367319107055664\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 29/100; Train Batch Loss: 4.1440653800964355\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 30/100; Train Batch Loss: 3.859065055847168\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 31/100; Train Batch Loss: 4.187124252319336\n",
      "Starting evaluating at epoch: 30\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 40\n",
      "pred idx: 51\n",
      "pred idx: 40\n",
      "pred idx: 5\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 32/100; Train Batch Loss: 4.226008415222168\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 33/100; Train Batch Loss: 4.23722505569458\n",
      "Current lr: 1e-05\n",
      "Step: 0/142; Epoch: 34/100; Train Batch Loss: 4.106267929077148\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 35/100; Train Batch Loss: 4.151429653167725\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 36/100; Train Batch Loss: 4.3868560791015625\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 37/100; Train Batch Loss: 3.960094451904297\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 38/100; Train Batch Loss: 4.172788619995117\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 39/100; Train Batch Loss: 4.178872108459473\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 40/100; Train Batch Loss: 4.2746124267578125\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 41/100; Train Batch Loss: 4.255580425262451\n",
      "Starting evaluating at epoch: 40\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 36\n",
      "pred idx: 8\n",
      "pred idx: 8\n",
      "pred idx: 8\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 42/100; Train Batch Loss: 4.370487213134766\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 43/100; Train Batch Loss: 4.301896095275879\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 44/100; Train Batch Loss: 4.210195541381836\n",
      "Current lr: 1.0000000000000002e-06\n",
      "Step: 0/142; Epoch: 45/100; Train Batch Loss: 4.285567760467529\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 46/100; Train Batch Loss: 4.263617038726807\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 47/100; Train Batch Loss: 4.336492538452148\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 48/100; Train Batch Loss: 4.388842582702637\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 49/100; Train Batch Loss: 4.187223434448242\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 50/100; Train Batch Loss: 4.06211519241333\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 51/100; Train Batch Loss: 4.068426132202148\n",
      "Starting evaluating at epoch: 50\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 61\n",
      "pred idx: 61\n",
      "pred idx: 59\n",
      "pred idx: 61\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 52/100; Train Batch Loss: 4.328061580657959\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 53/100; Train Batch Loss: 4.277115821838379\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 54/100; Train Batch Loss: 4.116232872009277\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 55/100; Train Batch Loss: 4.142367362976074\n",
      "Current lr: 1.0000000000000002e-07\n",
      "Step: 0/142; Epoch: 56/100; Train Batch Loss: 4.184285640716553\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 57/100; Train Batch Loss: 4.085467338562012\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 58/100; Train Batch Loss: 4.19541597366333\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 59/100; Train Batch Loss: 4.275121688842773\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 60/100; Train Batch Loss: 4.13192892074585\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 61/100; Train Batch Loss: 4.416557788848877\n",
      "Starting evaluating at epoch: 60\n",
      "Preparing CAM\n",
      "Layer1\n",
      "Layer2\n",
      "Layer3\n",
      "Layer4\n",
      "pred idx: 50\n",
      "pred idx: 55\n",
      "pred idx: 47\n",
      "pred idx: 55\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 62/100; Train Batch Loss: 4.251977443695068\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 63/100; Train Batch Loss: 4.1706976890563965\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 64/100; Train Batch Loss: 4.3087873458862305\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 65/100; Train Batch Loss: 4.140650749206543\n",
      "Current lr: 1.0000000000000004e-08\n",
      "Step: 0/142; Epoch: 66/100; Train Batch Loss: 4.389774799346924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b5106fc910a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m custom_model.train(loss_func = criterion, optimizer = optimizer, lr_scheduler = lr_scheduler, \n\u001b[1;32m     10\u001b[0m            \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m            valloader = val_loader, eval_period = 10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e89d4644e7dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loss_func, optimizer, lr_scheduler, learning_rate, epochs, trainloader, valloader, eval_period)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# train loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Barry/dissertation/shoe_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistortion_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mperspective\u001b[0;34m(img, startpoints, endpoints, interpolation, fill)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_perspective_coeffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPERSPECTIVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[1;32m   2412\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m             im.__transformer(\n\u001b[0;32m-> 2414\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillcolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m             )\n\u001b[1;32m   2416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__transformer\u001b[0;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2490\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Initialise the training of the model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# small eval period for debugging purposes\n",
    "\n",
    "custom_model.train(loss_func = criterion, optimizer = optimizer, lr_scheduler = lr_scheduler, \n",
    "           learning_rate = learning_rate, epochs = 100, trainloader = train_loader, \n",
    "           valloader = val_loader, eval_period = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trmf = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.RandomRotation(90)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image = Image.open(\"/Users/michalbarrington/Downloads/snake.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b030a5c4a1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_tens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, resample, expand, center, fill)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "img_tens = trmf(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
